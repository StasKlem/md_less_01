# LLM Chat Client

Консольное TUI-приложение на Go для общения с Large Language Models через API с поддержкой потокового вывода токенов.

## Возможности

- **Интерактивный TUI интерфейс** на базе Bubble Tea
- **Потоковый вывод** ответов (токены отображаются по мере поступления)
- **История диалога** с поддержкой контекста
- **Гибкая конфигурация** через флаги командной строки
- **Команды в чате** для изменения параметров на лету
- **Поддержка API** совместимых с OpenAI (Ollama, vLLM, и др.)

## Структура проекта

```
.
├── main.go          # Точка входа, инициализация приложения
├── config.go        # Конфигурация и парсинг флагов
├── messages.go      # Модели данных (сообщения, история)
├── client.go        # HTTP-клиент для LLM API со стримингом
├── ui.go            # Bubble Tea TUI компоненты
├── go.mod           # Зависимости Go модуля
└── go.sum           # Хеш-суммы зависимостей
```

## Архитектура

Приложение использует чистую архитектуру с разделением ответственности:

1. **UI Layer** (`ui.go`) — Bubble Tea модель, обработка ввода, рендеринг
2. **Client Layer** (`client.go`) — HTTP-запросы, парсинг SSE-стрима
3. **Domain Layer** (`messages.go`) — модели данных, история диалога
4. **Config Layer** (`config.go`) — конфигурация, валидация, флаги

## Сборка и запуск

### Требования

- Go 1.21 или новее

### Установка зависимостей

```bash
go mod tidy
```

### Сборка

```bash
go build -o llm-client .
```

### Запуск

```bash
# Базовый запуск (подключение к локальному Ollama)
./llm-client

# С указанием адреса сервера
./llm-client --address http://localhost:11434

# С полной конфигурацией
./llm-client \
  --address http://localhost:11434 \
  --model llama3 \
  --temperature 0.7 \
  --top-p 0.9 \
  --system "You are a helpful assistant."

# С API ключом (для облачных провайдеров)
./llm-client --api-key $OPENAI_API_KEY
```

### Переменные окружения

| Переменная | Описание |
|------------|----------|
| `ROUTERAI_API_KEY` | API ключ для аутентификации |
| `LLM_CLIENT_LOG` | Путь к файлу логов (опционально, например `/tmp/llm-client.log`) |

## Флаги командной строки

| Флаг | Краткий | Описание | По умолчанию |
|------|---------|----------|--------------|
| `--address` | `-a` | Адрес LLM сервера | `http://localhost:11434` |
| `--api-key` | `-k` | API ключ | (из `LLM_API_KEY`) |
| `--model` | `-m` | Имя модели | `llama3` |
| `--system` | `-s` | Системный промпт | `You are a helpful assistant.` |
| `--temperature` | `-t` | Температура (0.0-2.0) | `0.7` |
| `--top-p` | `-p` | Top P параметр (0.0-1.0) | `0.9` |

## Команды в чате

Во время работы чата доступны следующие команды (начинаются с `/`):

| Команда | Описание |
|---------|----------|
| `/set <param> <value>` | Изменить параметр (temperature, top_p, model, system) |
| `/clear` | Очистить историю диалога |
| `/config` | Показать текущую конфигурацию |
| `/help` | Показать список команд |
| `/exit` | Выйти из приложения |

**Примеры:**

```
/set temperature 0.9
/set model mistral
/set top_p 0.95
```

## Управление в интерфейсе

| Клавиша | Действие |
|---------|----------|
| `Enter` | Отправить сообщение |
| `↑` / `↓` | Скролл истории |
| `PgUp` / `PgDn` | Быстрый скролл |
| `Ctrl+C` | Прервать генерацию / Выход |

## Индикаторы статуса

- **○ Ожидание** — готов к вводу
- **● Отправка...** — запрос отправляется
- **● Печатает...** — получение ответа (стриминг)
- **✗ Ошибка** — ошибка сети или API

## Примеры использования

### Подключение к Ollama

```bash
# Убедитесь что Ollama запущен
ollama serve

# Запустите клиент
./llm-client --model llama3
```

### Подключение к OpenAI API

```bash
./llm-client \
  --address https://api.openai.com \
  --api-key $OPENAI_API_KEY \
  --model gpt-3.5-turbo
```

### Подключение к vLLM

```bash
./llm-client \
  --address http://localhost:8000 \
  --model meta-llama/Llama-2-7b-chat-hf
```

## Зависимости

- [bubbletea](https://github.com/charmbracelet/bubbletea) — TUI фреймворк
- [lipgloss](https://github.com/charmbracelet/lipgloss) — стилизация терминала

## Лицензия

MIT
